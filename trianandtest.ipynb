{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python392jvsc74a57bd019c51b98106890301ae58c21f0653689a3d53530e730b52321c83bfc13b5cd73",
   "display_name": "Python 3.9.2 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "19c51b98106890301ae58c21f0653689a3d53530e730b52321c83bfc13b5cd73"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'d:\\\\Github\\\\django-shop\\\\handpose-mouse-simulation\\\\data_all\\\\data\\\\diff'"
      ]
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "data_path = os.path.abspath('./data_all/data/diff/')\n",
    "data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "resList = []\n",
    "df_list = []\n",
    "label_list = []\n",
    "for filename in os.listdir(data_path):\n",
    "    if filename[-3:] ==\"csv\":\n",
    "        new_path = data_path + \"\\\\\"+ filename\n",
    "        resList.append(new_path)\n",
    "        label_list.append(filename[:-4])\n",
    "        df = pd.read_csv(new_path , delimiter=',', header=0, skiprows=0,\n",
    "                                   error_bad_lines=False)\n",
    "        #df_list.append(df)\n",
    "        df_list.extend(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Chinese_seven_diff',\n",
       " 'Click_diff',\n",
       " 'Ok_pose_diff',\n",
       " 'Relax_diff',\n",
       " 'Rock_diff',\n",
       " 'Up_down_diff']"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset():\n",
    "    def __init__(self, data, lable):\n",
    "        self.data = data\n",
    "        self.lable = lable\n",
    "        self.len = len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<__main__.dataset at 0x22889f46970>"
      ]
     },
     "metadata": {},
     "execution_count": 103
    }
   ],
   "source": [
    "#d1_train = dataset(df_list[0],label_list[0])\n",
    "d1_train = dataset(df_list,label_list[0])\n",
    "d1_train.lable\n",
    "d1_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# import the libraries for classification modelling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, mean_squared_error\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNLSTMNet(nn.Module):\n",
    "    def __init__(self, cnn_model, output_dim, hidden_dim,num_classes=10,seq_len=20 ,batch_size=1, num_lstm_layers = 1, bidirectional = False, device = 'cpu', freeze_layers=True, dropout=0, title=\"default\"):\n",
    "        super(CNNLSTMNet, self).__init__()\n",
    "        # CNN\n",
    "        self.device = device\n",
    "        self.title = title # Model Title\n",
    "        self.cnn_model = cnn_model # Torchvision CNN Model\n",
    "        \n",
    "        # Optionally Freeze CNN Layers\n",
    "        if freeze_layers:\n",
    "            for idxc, child in enumerate(self.cnn_model.children()):\n",
    "                for param in child.parameters():\n",
    "                    param.requires_grad = False\n",
    "            self.cnn_model.fc.requires_grad = True\n",
    "            \n",
    "        # RNN\n",
    "        self.seq_len = seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_dim = hidden_dim # LSTM Hidden Dimension Size\n",
    "        self.num_lstm_layers = num_lstm_layers \n",
    "        self.bidirectional = bidirectional # Sets LSTM to Uni or Bidirectional\n",
    "        self.bidirectional_mult = 2 if self.bidirectional else 1 # Used for LSTM Weight Shape\n",
    "        self.lstm = nn.LSTM(output_dim, hidden_dim, self.num_lstm_layers, bidirectional=self.bidirectional, dropout=dropout)\n",
    "        self.hidden2class = nn.Linear(hidden_dim*self.bidirectional_mult, num_classes) # Fully Connected Output Layer\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # (num_layers, minibatch_size, hidden_dim)\n",
    "        return (torch.zeros(self.num_lstm_layers*self.bidirectional_mult, self.batch_size, self.hidden_dim).to(device),\n",
    "                    torch.zeros(self.num_lstm_layers*self.bidirectional_mult, self.batch_size, self.hidden_dim).to(device))\n",
    "                    \n",
    "    def forward(self, x):\n",
    "        x = x.view(self.seq_len*self.batch_size,x.shape[-3],x.shape[-2],-1)\n",
    "        out = self.cnn_model(x)\n",
    "        seq = out.view(self.batch_size, self.seq_len, -1).transpose_(0,1)\n",
    "        self.hidden = self.init_hidden()\n",
    "        # LSTM input shape = (seq_len, batch, input_size)\n",
    "        out, self.hidden = self.lstm(seq.view(len(seq), self.batch_size, -1), self.hidden)\n",
    "        #LSTM output shape = (seq_len, batch, hidden_dim * bidirectional)\n",
    "        out = self.hidden2class(out[-1])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_img_features = 1024 # CNN output dimensions\n",
    "num_epochs = 10\n",
    "sequence_len = d1_train.shape # LSTM sequence length\n",
    "batch_size=3\n",
    "hidden_dim = 128 # LSTM hidden dimension size\n",
    "lstm_dropout = .1\n",
    "lstm_depth = 1\n",
    "freeze = False # True = Freeze entire CNN, False = Don't freeze any layers\n",
    "pretrain = True # Use Imagenet Pretraining with CNN\n",
    "lstm_depth_title = 'no_freeze_layers_num_lstm_layers_'+str(lstm_depth)\n",
    "num_classes = len(label_list)+1\n",
    "loss_fn = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-dataset-test-frozen-False-num_img_features-1024-num_epochs-10-sequence_len-379-batch_size-3-lstm_dim-128-lstm_depth-1-pretrain-TrueCNN-resnet18\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "title =\\\n",
    "'-dataset-'+str(\"test\")+\\\n",
    "'-frozen-'+str(freeze)+\\\n",
    "'-num_img_features-'+ str(num_img_features) +\\\n",
    "'-num_epochs-'+str(num_epochs)+\\\n",
    "'-sequence_len-'+str(sequence_len)+\\\n",
    "'-batch_size-'+str(batch_size)+\\\n",
    "'-lstm_dim-'+str(hidden_dim)+\\\n",
    "'-lstm_depth-'+str(lstm_depth)+\\\n",
    "'-pretrain-'+str(pretrain)+\\\n",
    "'CNN-resnet18'\n",
    "\n",
    "cnn_model = models.resnet18(pretrained=pretrain) #Choose different CNN if desired\n",
    "num_ftrs = cnn_model.fc.in_features\n",
    "cnn_model.fc = nn.Linear(num_ftrs, num_img_features) # Change CNN output layer to desired dimension\n",
    "model = CNNLSTMNet(cnn_model, num_img_features, hidden_dim, num_classes, sequence_len, batch_size, num_lstm_layers=lstm_depth, bidirectional=False, device=device, freeze_layers=freeze, dropout=lstm_dropout)\n",
    "optimizer = optim.Adam(model.parameters(), lr = 1e-4)\n",
    "model_save_path= 'saved_models/'+title+'.pth'\n",
    "# Use Below to load train history and train over a saved model\n",
    "# model.load_state_dict(torch.load(model_save_path))\n",
    "print(title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "#from plot_model_stats import *\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torchnet\n",
    "from torchnet.meter import ConfusionMeter\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def my_collate(batch):\n",
    "    # Filters DataLoader items after they are retrieved\n",
    "    data = [item[0] for item in batch if item != None]\n",
    "    target = [item[1] for item in batch if item != None]\n",
    "    if len(data) == 0: return [None, None]\n",
    "    data = torch.stack(data)\n",
    "    target = torch.LongTensor(target)\n",
    "    return [data, target]\n",
    "\n",
    "\n",
    "def train_model(model, loss_fn, batch_size, dataset, optimizer, model_title='None', device='cpu', root_dir='', num_epochs = 2, testset=None):\n",
    "    def write_results(title, write_string):\n",
    "        with open('model_results/'+title + '_results.txt', 'a+') as f:\n",
    "            print(write_string)\n",
    "            f.write(write_string + '\\n')\n",
    "\n",
    "    # Shuffling is needed in case dataset is not shuffled by default.\n",
    "    train_ratio=.9\n",
    "    train_len = int(len(dataset)*train_ratio)\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_len, len(dataset)-train_len]) \n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                               batch_size=batch_size,\n",
    "                                               collate_fn=my_collate\n",
    "                                               )\n",
    "    # We don't need to bach the validation set but let's do it anyway.\n",
    "    val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                             batch_size=batch_size,\n",
    "                                             collate_fn=my_collate\n",
    "                                             )  \n",
    "\n",
    "    # GPU enabling.\n",
    "    model = model.to(device)\n",
    "    loss_fn = loss_fn.to(device)\n",
    "\n",
    "    # Training loop. Please make sure you understand every single line of code below.\n",
    "    # Go back to some of the previous steps in this lab if necessary.\n",
    "    start_time = time.time()\n",
    "    plot_path=os.path.join(root_dir,'model_results/'+model_title+'.png')\n",
    "    print(\"Trainset Length: \", len(train_loader))\n",
    "    print(\"Valset Length: \", len(val_loader))\n",
    "    print(\"Epoch Count: \", num_epochs)\n",
    "    print(\"Plot Path: \",plot_path)\n",
    "    \n",
    "    train_accuracies=[]; val_accuracies=[]; train_losses=[]; val_losses=[]\n",
    "    for epoch in range(0, num_epochs):\n",
    "        correct = 0.0\n",
    "        cum_loss = 0.0\n",
    "\n",
    "        # Make a pass over the training data.\n",
    "        model.train()\n",
    "        num_trained = 1.0\n",
    "        confusion_matrix = torch.zeros(len(dataset.labels), len(dataset.labels))\n",
    "        for (i, (inputs, labels)) in enumerate(train_loader):\n",
    "            try:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)#[0]\n",
    "\n",
    "                # Forward pass. (Prediction stage)\n",
    "                scores = model(inputs)\n",
    "                loss = loss_fn(scores, labels)\n",
    "\n",
    "                # Zero the gradients in the network.\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Backward pass. (Gradient computation stage)\n",
    "                loss.backward()\n",
    "\n",
    "                # Parameter updates (SGD step) -- if done with torch.optim!\n",
    "                optimizer.step()\n",
    "\n",
    "                scores = F.softmax(scores)\n",
    "                max_scores, max_labels = scores.max(1)\n",
    "                correct_eval = (max_labels == labels).sum().item()\n",
    "                correct+= correct_eval\n",
    "                num_trained+=batch_size\n",
    "                cum_loss += loss.item()\n",
    "                _, preds = torch.max(scores, 1)\n",
    "                for t, p in zip(labels.view(-1), preds.view(-1)):\n",
    "                    confusion_matrix[t.long(), p.long()] += 1\n",
    "\n",
    "\n",
    "                # Logging the current results on training.\n",
    "                if (i + 1) % 100 == 0:\n",
    "                    print(dataset.labels)\n",
    "                    write_string =('Train-epoch %d. Iteration %05d, Avg-Loss: %.4f, Accuracy: %.4f' %\n",
    "                          (epoch, num_trained + 1, cum_loss / (num_trained), correct / (num_trained)))\n",
    "                    write_string += '\\n'+str(confusion_matrix)\n",
    "                    confusion_avg = (confusion_matrix.diag()/confusion_matrix.sum(1))\n",
    "                    write_string += '\\n'+ str(confusion_avg)\n",
    "                    write_results(model_title, write_string)\n",
    "                    #print(\"Time Elapsed Minutes: \", (time.time() - start_time) /60)\n",
    "                    correct = 0.0\n",
    "                    cum_loss = 0.0\n",
    "                    num_trained = 1.0\n",
    "                    confusion_matrix = torch.zeros(len(dataset.labels), len(dataset.labels))\n",
    "                    model_save_path= 'saved_models/'+model_title+'.pth'\n",
    "\n",
    "                    torch.save(model.state_dict(), model_save_path)\n",
    "            except Exception as e:\n",
    "                \n",
    "                print(e)\n",
    "        train_accuracies.append(correct / num_trained)\n",
    "        train_losses.append(cum_loss / num_trained)\n",
    "        write_string = \"Time Elapsed Minutes: \"+ str((time.time() - start_time) /60)\n",
    "        write_results(model_title, write_string)\n",
    "\n",
    "        # Make a pass over the validation data.\n",
    "        model.eval()\n",
    "        num_trained = 1.0\n",
    "        correct = 0.0\n",
    "        cum_loss = 0.0\n",
    "\n",
    "        confusion_matrix = torch.zeros(len(label_list), len(label_list))\n",
    "        print('Validating...')\n",
    "        print(dataset.labels)\n",
    "        for (i, (inputs, labels)) in enumerate(val_loader):\n",
    "            try:\n",
    "  \n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)#[0]\n",
    "\n",
    "                # Forward pass. (Prediction stage)\n",
    "                scores = model(inputs)\n",
    "                latent_scores.append([scores.item(),labels.item()])\n",
    "                loss = loss_fn(scores, labels)\n",
    "                scores = F.softmax(scores)\n",
    "                # Count how many correct in this batch.\n",
    "                max_scores, max_labels = scores.max(1)\n",
    "                correct_eval = (max_labels == labels).sum().item()\n",
    "                correct+= correct_eval\n",
    "                num_trained+=batch_size\n",
    "                cum_loss += loss.item()\n",
    "                _, preds = torch.max(scores, 1)\n",
    "                for t, p in zip(labels.view(-1), preds.view(-1)):\n",
    "                    confusion_matrix[t.long(), p.long()] += 1\n",
    "                if i % 100 == 0:\n",
    "                    print(confusion_matrix)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "        val_accuracies.append(correct / num_trained)\n",
    "        val_losses.append(cum_loss / num_trained)\n",
    "        write_string =('validation-epoch %d. Iteration %05d, Avg-Loss: %.4f, Accuracy: %.4f' %\n",
    "              (epoch, num_trained + 1, cum_loss / num_trained, correct / num_trained ))\n",
    "        write_string += '\\n'+str(confusion_matrix)\n",
    "        confusion_avg = (confusion_matrix.diag()/confusion_matrix.sum(1))\n",
    "        write_string += '\\n'+ str(confusion_avg)\n",
    "        write_results(model_title, write_string)\n",
    "        # plot_model_stats(train_accuracies, val_accuracies, train_losses, val_losses, plot_path, latent_scores=latent_scores)\n",
    "        if testset != None:\n",
    "            try:\n",
    "                from test_model import test_model\n",
    "                test_model(model,testset,dataset, batch_size, joint_run=joint_run)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "object of type 'dataset' has no len()",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-100-51091af1b741>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md1_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-96-1000f770e454>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, loss_fn, batch_size, dataset, optimizer, model_title, device, root_dir, num_epochs, testset)\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;31m# Shuffling is needed in case dataset is not shuffled by default.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mtrain_ratio\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m.9\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m     \u001b[0mtrain_len\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtrain_ratio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m     \u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtrain_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mtrain_len\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'dataset' has no len()"
     ]
    }
   ],
   "source": [
    "train_model(model, loss_fn, batch_size, d1_train.data, optimizer, title, device, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}