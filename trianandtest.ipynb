{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python392jvsc74a57bd019c51b98106890301ae58c21f0653689a3d53530e730b52321c83bfc13b5cd73",
   "display_name": "Python 3.9.2 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "19c51b98106890301ae58c21f0653689a3d53530e730b52321c83bfc13b5cd73"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'d:\\\\Github\\\\django-shop\\\\handpose-mouse-simulation\\\\data_all\\\\data\\\\diff'"
      ]
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "data_path = os.path.abspath('./data_all/data/diff/')\n",
    "data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "resList = []\n",
    "df_list = []\n",
    "label_list = []\n",
    "for filename in os.listdir(data_path):\n",
    "    if filename[-3:] ==\"csv\":\n",
    "        new_path = data_path + \"\\\\\"+ filename\n",
    "        resList.append(new_path)\n",
    "        label_list.append(filename[:-4])\n",
    "        df = pd.read_csv(new_path , delimiter=',', header=0, skiprows=0,\n",
    "                                   error_bad_lines=False)\n",
    "        df_list.append(df)\n",
    "        #df_list.extend(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Chinese_seven_diff',\n",
       " 'Click_diff',\n",
       " 'Ok_pose_diff',\n",
       " 'Relax_diff',\n",
       " 'Rock_diff',\n",
       " 'Up_down_diff']"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset():\n",
    "    def __init__(self, data, lable):\n",
    "        self.data = data\n",
    "        self.lable = lable\n",
    "        self.len = len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[      -0.00033479928970336914  -0.0023109912872314453  6.41061706119217e-06  \\\n",
       " 0                    0.002057               -0.002193              0.000008   \n",
       " 1                   -0.003012               -0.003290             -0.000017   \n",
       " 2                   -0.000389               -0.002900              0.000008   \n",
       " 3                   -0.002330               -0.000561             -0.000016   \n",
       " 4                    0.000000               -0.003600              0.000009   \n",
       " ...                       ...                     ...                   ...   \n",
       " 2602                 0.007303                0.002958             -0.000007   \n",
       " 2603                 0.412453                0.260215             -0.000019   \n",
       " 2604                -0.347082               -0.313046              0.000048   \n",
       " 2605                 0.009684                0.001036              0.000039   \n",
       " 2606                 0.000827               -0.003932             -0.000019   \n",
       " \n",
       "       0.00011456012725830078  0.001500546932220459  -0.0045203195186331  \\\n",
       " 0                   0.001567             -0.000057             0.001306   \n",
       " 1                  -0.002073             -0.002048            -0.001140   \n",
       " 2                  -0.001165             -0.000910            -0.002860   \n",
       " 3                  -0.000773             -0.000992             0.002435   \n",
       " 4                  -0.000731             -0.004086            -0.000003   \n",
       " ...                      ...                   ...                  ...   \n",
       " 2602                0.015802              0.007943            -0.002154   \n",
       " 2603                0.437732              0.234771            -0.032096   \n",
       " 2604               -0.372406             -0.270414             0.018490   \n",
       " 2605                0.000948             -0.011377             0.024573   \n",
       " 2606                0.005345              0.001425             0.000517   \n",
       " \n",
       "       0.0008445382118225098  -0.0007509589195251465  -0.0064512863755226  \\\n",
       " 0                  0.001426                0.000441             0.002046   \n",
       " 1                 -0.001753               -0.002264            -0.001044   \n",
       " 2                 -0.000912               -0.002214            -0.005039   \n",
       " 3                 -0.000130               -0.001949             0.005781   \n",
       " 4                 -0.000409               -0.003383            -0.000801   \n",
       " ...                     ...                     ...                  ...   \n",
       " 2602               0.021161                0.000276            -0.001874   \n",
       " 2603               0.445144                0.201135            -0.054270   \n",
       " 2604              -0.379387               -0.216514             0.031060   \n",
       " 2605              -0.002304               -0.009724             0.032534   \n",
       " 2606               0.006428                0.008444            -0.001244   \n",
       " \n",
       "       0.0005733966827392578  ...  -0.0007488355040550093  \\\n",
       " 0                  0.001535  ...               -0.000413   \n",
       " 1                 -0.000706  ...               -0.000686   \n",
       " 2                 -0.001763  ...               -0.003620   \n",
       " 3                  0.000559  ...                0.004609   \n",
       " 4                  0.000135  ...               -0.001531   \n",
       " ...                     ...  ...                     ...   \n",
       " 2602               0.022059  ...               -0.009710   \n",
       " 2603               0.440364  ...               -0.039009   \n",
       " 2604              -0.369851  ...                0.021983   \n",
       " 2605               0.001876  ...                0.020040   \n",
       " 2606               0.010530  ...                0.006828   \n",
       " \n",
       "       0.0020958781242370605  -0.0004208087921142578  -0.0033496394753455977  \\\n",
       " 0                  0.000754                0.000814               -0.000192   \n",
       " 1                  0.000396               -0.001182               -0.003524   \n",
       " 2                 -0.002667               -0.001837               -0.005470   \n",
       " 3                 -0.000942               -0.003688                0.003422   \n",
       " 4                  0.001237               -0.001998                0.001291   \n",
       " ...                     ...                     ...                     ...   \n",
       " 2602               0.020710               -0.001155               -0.012860   \n",
       " 2603               0.374099                0.178791               -0.071417   \n",
       " 2604              -0.304322               -0.195694                0.046080   \n",
       " 2605               0.008967                0.007945                0.037300   \n",
       " 2606               0.011666               -0.007985                0.007379   \n",
       " \n",
       "       0.0012412667274475098  -8.189678192138672e-05  -0.0022081583738327026  \\\n",
       " 0                  0.000307                0.000272               -0.000621   \n",
       " 1                 -0.000173               -0.000828               -0.003492   \n",
       " 2                 -0.002954               -0.002125               -0.004983   \n",
       " 3                 -0.001040               -0.002199                0.004529   \n",
       " 4                  0.001426               -0.003112                0.000118   \n",
       " ...                     ...                     ...                     ...   \n",
       " 2602               0.024060                0.003954               -0.007203   \n",
       " 2603               0.377592                0.197915               -0.080014   \n",
       " 2604              -0.318201               -0.203344                0.050886   \n",
       " 2605               0.013250               -0.012617                0.035719   \n",
       " 2606               0.014164                0.001100                0.004016   \n",
       " \n",
       "       0.0012236833572387695  -0.0007871389389038086  -0.0013976469635963995  \n",
       " 0                 -0.000284               -0.000245               -0.000962  \n",
       " 1                 -0.000986                0.001404               -0.002702  \n",
       " 2                 -0.002890               -0.002892               -0.003746  \n",
       " 3                 -0.000959               -0.000585                0.004779  \n",
       " 4                  0.001947               -0.005574               -0.000769  \n",
       " ...                     ...                     ...                     ...  \n",
       " 2602               0.019406                0.003481               -0.000895  \n",
       " 2603               0.382292                0.219114               -0.082187  \n",
       " 2604              -0.327375               -0.223684                0.050123  \n",
       " 2605               0.016926               -0.016643                0.031579  \n",
       " 2606               0.010339                0.004024               -0.001543  \n",
       " \n",
       " [2607 rows x 63 columns],\n",
       "       -0.002546548843383789  -0.00523066520690918  -8.240574970841408e-06  \\\n",
       " 0                 -0.000696             -0.002773           -3.974892e-06   \n",
       " 1                 -0.001689              0.000766            1.343441e-05   \n",
       " 2                 -0.001186             -0.000082           -3.777313e-08   \n",
       " 3                  0.000819              0.000235            2.468161e-06   \n",
       " 4                  0.000947              0.002376            5.747199e-06   \n",
       " ...                     ...                   ...                     ...   \n",
       " 3305               0.000000              0.000000            0.000000e+00   \n",
       " 3306               0.000000              0.000000            0.000000e+00   \n",
       " 3307               0.000000              0.000000            0.000000e+00   \n",
       " 3308               0.000000              0.000000            0.000000e+00   \n",
       " 3309               0.000000              0.000000            0.000000e+00   \n",
       " \n",
       "       -0.0015641450881958008  -9.971857070922852e-05  -0.0030116010457276986  \\\n",
       " 0                  -0.001434                0.000127                0.000316   \n",
       " 1                  -0.000514                0.003653               -0.002584   \n",
       " 2                  -0.000058                0.000763               -0.000563   \n",
       " 3                   0.000487                0.001690               -0.002520   \n",
       " 4                   0.001901                0.002389               -0.000824   \n",
       " ...                      ...                     ...                     ...   \n",
       " 3305                0.000000                0.000000                0.000000   \n",
       " 3306                0.000000                0.000000                0.000000   \n",
       " 3307                0.000000                0.000000                0.000000   \n",
       " 3308                0.000000                0.000000                0.000000   \n",
       " 3309                0.000000                0.000000                0.000000   \n",
       " \n",
       "       -0.00255468487739563  -0.0009658336639404297  -0.0015117488801478993  \\\n",
       " 0                -0.000995                0.000743                0.000838   \n",
       " 1                -0.000476                0.002349               -0.002523   \n",
       " 2                 0.000615                0.000448               -0.000841   \n",
       " 3                 0.000265                0.002068               -0.002260   \n",
       " 4                 0.002074                0.002227               -0.000947   \n",
       " ...                    ...                     ...                     ...   \n",
       " 3305              0.000000                0.000000                0.000000   \n",
       " 3306              0.000000                0.000000                0.000000   \n",
       " 3307              0.000000                0.000000                0.000000   \n",
       " 3308              0.000000                0.000000                0.000000   \n",
       " 3309              0.000000                0.000000                0.000000   \n",
       " \n",
       "       -0.003005474805831909  ...  0.004277925938367906  \\\n",
       " 0                 -0.000178  ...              0.001668   \n",
       " 1                  0.000129  ...             -0.000573   \n",
       " 2                  0.000441  ...             -0.001230   \n",
       " 3                  0.000175  ...              0.004872   \n",
       " 4                  0.002137  ...             -0.000297   \n",
       " ...                     ...  ...                   ...   \n",
       " 3305               0.000000  ...              0.000000   \n",
       " 3306               0.000000  ...              0.000000   \n",
       " 3307               0.000000  ...              0.000000   \n",
       " 3308               0.000000  ...              0.000000   \n",
       " 3309               0.000000  ...              0.000000   \n",
       " \n",
       "       -9.042024612426758e-05  0.0006397366523742676  0.00035481154918669267  \\\n",
       " 0                  -0.000778               0.002020                0.002088   \n",
       " 1                   0.000972               0.000461               -0.001669   \n",
       " 2                   0.000243               0.003502               -0.000463   \n",
       " 3                   0.000238               0.001309                0.005810   \n",
       " 4                   0.002599               0.000848               -0.000816   \n",
       " ...                      ...                    ...                     ...   \n",
       " 3305                0.000000               0.000000                0.000000   \n",
       " 3306                0.000000               0.000000                0.000000   \n",
       " 3307                0.000000               0.000000                0.000000   \n",
       " 3308                0.000000               0.000000                0.000000   \n",
       " 3309                0.000000               0.000000                0.000000   \n",
       " \n",
       "       -0.0006042718887329102  0.003861665725708008  -0.001921422779560103  \\\n",
       " 0                  -0.001374              0.002966               0.001637   \n",
       " 1                   0.000598              0.000976              -0.001416   \n",
       " 2                   0.000588              0.003087               0.000261   \n",
       " 3                   0.000757              0.001702               0.005679   \n",
       " 4                   0.001463              0.001511               0.000293   \n",
       " ...                      ...                   ...                    ...   \n",
       " 3305                0.000000              0.000000               0.000000   \n",
       " 3306                0.000000              0.000000               0.000000   \n",
       " 3307                0.000000              0.000000               0.000000   \n",
       " 3308                0.000000              0.000000               0.000000   \n",
       " 3309                0.000000              0.000000               0.000000   \n",
       " \n",
       "       -0.0015103816986083984  0.006600916385650635  -0.0015483573079108914  \n",
       " 0                  -0.002035              0.004013                0.001690  \n",
       " 1                   0.000711              0.002778               -0.000802  \n",
       " 2                   0.000791              0.002826                0.001004  \n",
       " 3                   0.001417              0.002066                0.005401  \n",
       " 4                   0.000473              0.002500                0.001103  \n",
       " ...                      ...                   ...                     ...  \n",
       " 3305                0.000000              0.000000                0.000000  \n",
       " 3306                0.000000              0.000000                0.000000  \n",
       " 3307                0.000000              0.000000                0.000000  \n",
       " 3308                0.000000              0.000000                0.000000  \n",
       " 3309                0.000000              0.000000                0.000000  \n",
       " \n",
       " [3310 rows x 63 columns],\n",
       "       -0.0007178187370300293  -0.0011682510375976562  -4.60201408714056e-06  \\\n",
       " 0                  -0.000153                0.000207          -2.887879e-06   \n",
       " 1                  -0.000016               -0.000245          -6.717382e-07   \n",
       " 2                  -0.001324                0.000587          -4.215162e-07   \n",
       " 3                  -0.000988                0.000488          -3.553638e-06   \n",
       " 4                  -0.000683                0.001145          -5.315916e-06   \n",
       " ...                      ...                     ...                    ...   \n",
       " 2617                0.009781               -0.016381          -1.925661e-05   \n",
       " 2618                0.010518               -0.011656          -1.543414e-06   \n",
       " 2619                0.011013               -0.007506          -2.235364e-05   \n",
       " 2620                0.005828               -0.007174           2.290388e-05   \n",
       " 2621                0.004504               -0.002498          -3.759549e-06   \n",
       " \n",
       "       0.0006677508354187012  -9.882450103759766e-05  0.00028350716456769943  \\\n",
       " 0                 -0.000509                0.000626               -0.002240   \n",
       " 1                  0.000598               -0.000386                0.001289   \n",
       " 2                 -0.000738                0.000596               -0.001570   \n",
       " 3                 -0.000567                0.000982                0.001606   \n",
       " 4                 -0.000053                0.000295               -0.000507   \n",
       " ...                     ...                     ...                     ...   \n",
       " 2617               0.011484               -0.013659                0.002027   \n",
       " 2618               0.012263               -0.011544               -0.000803   \n",
       " 2619               0.013470               -0.006885                0.004494   \n",
       " 2620               0.006281               -0.012739                0.004575   \n",
       " 2621               0.003445               -0.004119                0.000376   \n",
       " \n",
       "       0.0004950165748596191  0.00040644407272338867  0.0002880003303288997  \\\n",
       " 0                 -0.000473                0.000174              -0.001504   \n",
       " 1                  0.001010                0.000332               0.000656   \n",
       " 2                 -0.001328               -0.000486              -0.001193   \n",
       " 3                 -0.000290                0.001317               0.002586   \n",
       " 4                  0.000204                0.000872              -0.000822   \n",
       " ...                     ...                     ...                    ...   \n",
       " 2617               0.013621               -0.012764               0.003555   \n",
       " 2618               0.012394               -0.009365              -0.000862   \n",
       " 2619               0.015848               -0.007797               0.007813   \n",
       " 2620               0.007431               -0.016706               0.006553   \n",
       " 2621               0.001716               -0.004070               0.002162   \n",
       " \n",
       "       0.0005476176738739014  ...  -0.0007752776145934989  \\\n",
       " 0                  0.000014  ...                0.005309   \n",
       " 1                  0.000889  ...               -0.002993   \n",
       " 2                 -0.001103  ...                0.002844   \n",
       " 3                 -0.000251  ...                0.000095   \n",
       " 4                  0.000657  ...                0.001170   \n",
       " ...                     ...  ...                     ...   \n",
       " 2617               0.014450  ...               -0.002984   \n",
       " 2618               0.013663  ...               -0.001063   \n",
       " 2619               0.017903  ...                0.008260   \n",
       " 2620               0.007856  ...               -0.002487   \n",
       " 2621               0.002468  ...                0.005168   \n",
       " \n",
       "       -0.00016951560974121094  -0.0008107423782348633  -0.0016016624867915968  \\\n",
       " 0                    0.000398               -0.002842                0.006714   \n",
       " 1                    0.000829                0.002309               -0.003181   \n",
       " 2                   -0.000037               -0.000366                0.002982   \n",
       " 3                    0.000380                0.000328               -0.000055   \n",
       " 4                    0.000246                0.001468                0.001781   \n",
       " ...                       ...                     ...                     ...   \n",
       " 2617                 0.014408               -0.016681               -0.003793   \n",
       " 2618                 0.013175               -0.012153               -0.001842   \n",
       " 2619                 0.011679               -0.014452                0.011921   \n",
       " 2620                 0.008751               -0.008519               -0.005277   \n",
       " 2621                 0.008077               -0.007549                0.004566   \n",
       " \n",
       "       -0.0009132027626037598  -0.0012639164924621582  -0.0012277737259864044  \\\n",
       " 0                   0.000970               -0.003127                0.007566   \n",
       " 1                   0.000356                0.002625               -0.002939   \n",
       " 2                   0.000362               -0.000326                0.003246   \n",
       " 3                   0.000475               -0.000033               -0.000558   \n",
       " 4                   0.000531                0.001498                0.002296   \n",
       " ...                      ...                     ...                     ...   \n",
       " 2617                0.016455               -0.016156               -0.003505   \n",
       " 2618                0.014083               -0.011807               -0.001798   \n",
       " 2619                0.013276               -0.016384                0.015201   \n",
       " 2620                0.008785               -0.007039               -0.004448   \n",
       " 2621                0.008403               -0.006135                0.005499   \n",
       " \n",
       "       -0.0009999871253967285  -0.001799941062927246  -0.0010011307895182939  \n",
       " 0                   0.001628              -0.003818                0.007690  \n",
       " 1                   0.000061               0.003824               -0.002175  \n",
       " 2                   0.000920              -0.000687                0.003107  \n",
       " 3                   0.000499              -0.000396               -0.001277  \n",
       " 4                   0.000811               0.001577                0.003169  \n",
       " ...                      ...                    ...                     ...  \n",
       " 2617                0.019140              -0.015997               -0.001534  \n",
       " 2618                0.016345              -0.010979               -0.001528  \n",
       " 2619                0.014363              -0.016635                0.016762  \n",
       " 2620                0.008174              -0.007617               -0.002580  \n",
       " 2621                0.007640              -0.005112                0.006012  \n",
       " \n",
       " [2622 rows x 63 columns],\n",
       "       0.0016971826553344727  -9.250640869140625e-05  -7.646738595212803e-06  \\\n",
       " 0                  0.001431               -0.002267                0.000003   \n",
       " 1                  0.002437               -0.002072                0.000004   \n",
       " 2                 -0.000820               -0.002874                0.000013   \n",
       " 3                 -0.001940               -0.003074               -0.000005   \n",
       " 4                 -0.000771               -0.002766               -0.000004   \n",
       " ...                     ...                     ...                     ...   \n",
       " 2529              -0.001316                0.001482               -0.000002   \n",
       " 2530               0.000987                0.001854                0.000009   \n",
       " 2531               0.000225               -0.002510                0.000009   \n",
       " 2532               0.000912                0.002727               -0.000015   \n",
       " 2533               0.001247                0.003237               -0.000001   \n",
       " \n",
       "       0.001437067985534668  -0.0023760199546813965  0.0001918673515319963  \\\n",
       " 0                 0.001592               -0.003167               0.001227   \n",
       " 1                 0.001745               -0.001354              -0.000671   \n",
       " 2                -0.001327               -0.003646               0.000727   \n",
       " 3                -0.000826               -0.002342               0.000719   \n",
       " 4                -0.000511               -0.001862               0.000871   \n",
       " ...                    ...                     ...                    ...   \n",
       " 2529             -0.000549               -0.000369              -0.001707   \n",
       " 2530              0.000673                0.005278              -0.002495   \n",
       " 2531              0.000782               -0.004258               0.008064   \n",
       " 2532              0.001184                0.002793              -0.003035   \n",
       " 2533              0.001717                0.003310              -0.002739   \n",
       " \n",
       "       0.001999080181121826  -0.002798736095428467  -0.0008266530930996011  \\\n",
       " 0                 0.001218              -0.003321                0.001692   \n",
       " 1                 0.001550               0.000593               -0.000005   \n",
       " 2                -0.000208              -0.003156               -0.000532   \n",
       " 3                -0.000356              -0.003212                0.001729   \n",
       " 4                -0.000422              -0.001644                0.001593   \n",
       " ...                    ...                    ...                     ...   \n",
       " 2529             -0.000417              -0.001051               -0.002378   \n",
       " 2530              0.001077               0.005101               -0.004220   \n",
       " 2531              0.001112              -0.003550                0.013129   \n",
       " 2532              0.000580               0.003780               -0.005991   \n",
       " 2533              0.001413               0.002117               -0.003577   \n",
       " \n",
       "       0.002519369125366211  ...  -0.0021566972136498053  0.002314329147338867  \\\n",
       " 0                 0.001090  ...               -0.000128              0.001682   \n",
       " 1                 0.001708  ...                0.007641             -0.000129   \n",
       " 2                 0.000559  ...               -0.006669             -0.000483   \n",
       " 3                -0.000039  ...                0.001307             -0.000888   \n",
       " 4                -0.000477  ...                0.001178             -0.000272   \n",
       " ...                    ...  ...                     ...                   ...   \n",
       " 2529             -0.000501  ...                0.000096             -0.000256   \n",
       " 2530              0.000653  ...               -0.002837             -0.000612   \n",
       " 2531              0.001555  ...               -0.002230             -0.000450   \n",
       " 2532              0.000088  ...                0.002086              0.001809   \n",
       " 2533              0.001734  ...               -0.000626              0.001859   \n",
       " \n",
       "       0.0001392960548400879  -0.0030402243137359897  0.0021724700927734375  \\\n",
       " 0                 -0.001584               -0.000061               0.001224   \n",
       " 1                 -0.006497                0.009256               0.000133   \n",
       " 2                  0.001620               -0.008416              -0.000436   \n",
       " 3                 -0.003942                0.002410              -0.001393   \n",
       " 4                 -0.002421                0.001291               0.000258   \n",
       " ...                     ...                     ...                    ...   \n",
       " 2529               0.001586               -0.001164               0.002591   \n",
       " 2530               0.000257               -0.005534              -0.001999   \n",
       " 2531               0.000522               -0.002436              -0.000358   \n",
       " 2532               0.000452                0.005077               0.003099   \n",
       " 2533               0.001145               -0.001732               0.000263   \n",
       " \n",
       "       -0.0007715821266174316  -0.0027938485145568848  0.0022085905075073242  \\\n",
       " 0                  -0.001893                0.000074               0.001057   \n",
       " 1                  -0.004557                0.010005               0.000173   \n",
       " 2                  -0.000053               -0.009442               0.000421   \n",
       " 3                  -0.003885                0.003894              -0.002157   \n",
       " 4                  -0.002407                0.001591               0.000604   \n",
       " ...                      ...                     ...                    ...   \n",
       " 2529                0.000971               -0.004335               0.006165   \n",
       " 2530               -0.001009               -0.006969              -0.003022   \n",
       " 2531                0.002428               -0.002133              -0.000492   \n",
       " 2532               -0.001402                0.005068               0.004517   \n",
       " 2533                0.002142               -0.000221              -0.001276   \n",
       " \n",
       "       -0.0028713345527648926  -0.002157479524612399  \n",
       " 0                  -0.003342               0.000475  \n",
       " 1                  -0.001565               0.011230  \n",
       " 2                  -0.003061              -0.010433  \n",
       " 3                  -0.002950               0.005073  \n",
       " 4                  -0.002984               0.001892  \n",
       " ...                      ...                    ...  \n",
       " 2529                0.001886              -0.007227  \n",
       " 2530               -0.003166              -0.008272  \n",
       " 2531                0.004672              -0.000561  \n",
       " 2532               -0.003029               0.003836  \n",
       " 2533                0.002065               0.001050  \n",
       " \n",
       " [2534 rows x 63 columns],\n",
       "       0.0005900263786315918  0.0031489133834838867  5.150490324012935e-06  \\\n",
       " 0                  0.000397               0.000033           1.385844e-06   \n",
       " 1                 -0.000362               0.000423           2.937159e-07   \n",
       " 2                  0.000863              -0.000796          -6.758230e-06   \n",
       " 3                 -0.000190               0.001114          -8.578354e-09   \n",
       " 4                  0.000118               0.000429          -1.944813e-06   \n",
       " ...                     ...                    ...                    ...   \n",
       " 3227              -0.000156              -0.002609          -4.626854e-07   \n",
       " 3228               0.000720              -0.005204          -6.918761e-06   \n",
       " 3229               0.000770               0.001984           7.640047e-07   \n",
       " 3230               0.473220               0.564532          -1.876131e-05   \n",
       " 3231               0.000000               0.000000           0.000000e+00   \n",
       " \n",
       "       -0.0006929636001586914  0.00026351213455200195  0.0016496405005455  \\\n",
       " 0                   0.000239               -0.000635           -0.000495   \n",
       " 1                  -0.000182                0.000413           -0.000218   \n",
       " 2                   0.000229               -0.000595           -0.001123   \n",
       " 3                   0.000099                0.000544            0.001297   \n",
       " 4                  -0.000067                0.000319           -0.000700   \n",
       " ...                      ...                     ...                 ...   \n",
       " 3227                0.000737               -0.002489           -0.012578   \n",
       " 3228                0.002784               -0.005632            0.000379   \n",
       " 3229                0.003533                0.002902            0.007525   \n",
       " 3230                0.481427                0.584163           -0.024297   \n",
       " 3231                0.000000                0.000000            0.000000   \n",
       " \n",
       "       -0.001031041145324707  -0.0009203553199768066  0.0026969853788613996  \\\n",
       " 0                  0.000941               -0.000031              -0.001466   \n",
       " 1                 -0.000305               -0.000014              -0.000067   \n",
       " 2                 -0.000127                0.000244              -0.001641   \n",
       " 3                 -0.000135                0.000454               0.001134   \n",
       " 4                  0.000275               -0.000215              -0.000880   \n",
       " ...                     ...                     ...                    ...   \n",
       " 3227               0.003273               -0.003995              -0.018469   \n",
       " 3228               0.003270               -0.004339              -0.000767   \n",
       " 3229               0.003503                0.001917               0.009304   \n",
       " 3230               0.494896                0.598098              -0.040309   \n",
       " 3231               0.000000                0.000000               0.000000   \n",
       " \n",
       "       -0.0002524256706237793  ...  -0.0035784542560576976  \\\n",
       " 0                   0.001116  ...                0.000426   \n",
       " 1                  -0.000125  ...                0.001166   \n",
       " 2                  -0.000401  ...                0.000317   \n",
       " 3                  -0.000611  ...               -0.003459   \n",
       " 4                   0.000613  ...                0.000009   \n",
       " ...                      ...  ...                     ...   \n",
       " 3227                0.006959  ...               -0.003968   \n",
       " 3228                0.004616  ...               -0.003374   \n",
       " 3229                0.003672  ...               -0.003635   \n",
       " 3230                0.505900  ...               -0.032988   \n",
       " 3231                0.000000  ...                0.000000   \n",
       " \n",
       "       0.001089632511138916  0.001428365707397461  -0.0054306015372277  \\\n",
       " 0                 0.000110              0.000124             0.001762   \n",
       " 1                 0.000177             -0.000924             0.001556   \n",
       " 2                -0.001422             -0.002257            -0.000870   \n",
       " 3                 0.000622              0.002228            -0.002800   \n",
       " 4                 0.000294              0.000067            -0.000293   \n",
       " ...                    ...                   ...                  ...   \n",
       " 3227              0.000053              0.008758            -0.006397   \n",
       " 3228              0.002090              0.008675             0.000365   \n",
       " 3229             -0.001935             -0.003167            -0.000227   \n",
       " 3230              0.518490              0.559544            -0.044350   \n",
       " 3231              0.000000              0.000000             0.000000   \n",
       " \n",
       "       0.0010452866554260254  -0.001719057559967041  -0.006061635911464705  \\\n",
       " 0                  0.000142               0.001101               0.001814   \n",
       " 1                  0.000084              -0.000013               0.002293   \n",
       " 2                 -0.001472              -0.000586              -0.002443   \n",
       " 3                 -0.000061               0.000637              -0.001605   \n",
       " 4                  0.000304               0.000393              -0.000373   \n",
       " ...                     ...                    ...                    ...   \n",
       " 3227              -0.000927               0.009495              -0.009204   \n",
       " 3228               0.002951               0.008689               0.003081   \n",
       " 3229              -0.001054              -0.001556               0.000424   \n",
       " 3230               0.508970               0.564422              -0.046094   \n",
       " 3231               0.000000               0.000000               0.000000   \n",
       " \n",
       "       0.0005500912666320801  -0.004456162452697754  -0.005396664142608601  \n",
       " 0                 -0.000019               0.002087               0.001411  \n",
       " 1                  0.000407               0.000230               0.002783  \n",
       " 2                 -0.000590               0.000522              -0.003279  \n",
       " 3                 -0.000579              -0.001228              -0.000712  \n",
       " 4                  0.000173               0.000716              -0.000344  \n",
       " ...                     ...                    ...                    ...  \n",
       " 3227               0.000013               0.009013              -0.012675  \n",
       " 3228               0.001933               0.006721               0.003789  \n",
       " 3229              -0.001607              -0.001574               0.000365  \n",
       " 3230               0.503469               0.564051              -0.044573  \n",
       " 3231               0.000000               0.000000               0.000000  \n",
       " \n",
       " [3232 rows x 63 columns],\n",
       "       -0.010063111782073975  0.0034773349761963446  1.2137399608036503e-05  \\\n",
       " 0                  0.002366              -0.012014               -0.000018   \n",
       " 1                 -0.005466              -0.017512               -0.000010   \n",
       " 2                  0.004695              -0.027812               -0.000017   \n",
       " 3                  0.537147               0.478084               -0.000023   \n",
       " 4                  0.000000               0.000000                0.000000   \n",
       " ...                     ...                    ...                     ...   \n",
       " 2515              -0.006016              -0.001723               -0.000209   \n",
       " 2516               0.480604               0.670627                0.000121   \n",
       " 2517               0.000000               0.000000                0.000000   \n",
       " 2518               0.000000               0.000000                0.000000   \n",
       " 2519               0.000000               0.000000                0.000000   \n",
       " \n",
       "       -0.005362898111343384  0.012083381414413508  -0.006496114656329197  \\\n",
       " 0                  0.002277             -0.015636               0.005424   \n",
       " 1                 -0.005304             -0.019152               0.009041   \n",
       " 2                  0.001357             -0.030682               0.000126   \n",
       " 3                  0.493455              0.488700              -0.036766   \n",
       " 4                  0.000000              0.000000               0.000000   \n",
       " ...                     ...                   ...                    ...   \n",
       " 2515              -0.008669              0.012606               0.025639   \n",
       " 2516               0.516759              0.686307              -0.059437   \n",
       " 2517               0.000000              0.000000               0.000000   \n",
       " 2518               0.000000              0.000000               0.000000   \n",
       " 2519               0.000000              0.000000               0.000000   \n",
       " \n",
       "       -0.0049060881137847345  0.016984283924102783  -0.0126872360706329  \\\n",
       " 0                   0.001212             -0.016497             0.008653   \n",
       " 1                  -0.001852             -0.025708             0.014952   \n",
       " 2                  -0.002538             -0.034596             0.006008   \n",
       " 3                   0.456670              0.462541            -0.063160   \n",
       " 4                   0.000000              0.000000             0.000000   \n",
       " ...                      ...                   ...                  ...   \n",
       " 2515               -0.016291              0.036923             0.050702   \n",
       " 2516                0.541236              0.703618            -0.120095   \n",
       " 2517                0.000000              0.000000             0.000000   \n",
       " 2518                0.000000              0.000000             0.000000   \n",
       " 2519                0.000000              0.000000             0.000000   \n",
       " \n",
       "       -0.003788471221923828  ...  -0.008862435817718506  0.00622564554214472  \\\n",
       " 0                 -0.001076  ...               0.001108            -0.009832   \n",
       " 1                  0.000973  ...               0.006426            -0.018162   \n",
       " 2                 -0.005563  ...               0.006140            -0.031934   \n",
       " 3                  0.438220  ...               0.502629             0.294775   \n",
       " 4                  0.000000  ...               0.000000             0.000000   \n",
       " ...                     ...  ...                    ...                  ...   \n",
       " 2515              -0.020522  ...               0.004277             0.003750   \n",
       " 2516               0.553425  ...               0.411712             0.701566   \n",
       " 2517               0.000000  ...               0.000000             0.000000   \n",
       " 2518               0.000000  ...               0.000000             0.000000   \n",
       " 2519               0.000000  ...               0.000000             0.000000   \n",
       " \n",
       "       -0.0115433596074581  -0.01071652770042425  0.005654543638229398  \\\n",
       " 0                0.006681             -0.000565             -0.009139   \n",
       " 1                0.007598              0.007354             -0.018273   \n",
       " 2                0.014829              0.006409             -0.031869   \n",
       " 3               -0.070734              0.490979              0.264186   \n",
       " 4                0.000000              0.000000              0.000000   \n",
       " ...                   ...                   ...                   ...   \n",
       " 2515             0.025768             -0.012296              0.020473   \n",
       " 2516            -0.229308              0.416160              0.719275   \n",
       " 2517             0.000000              0.000000              0.000000   \n",
       " 2518             0.000000              0.000000              0.000000   \n",
       " 2519             0.000000              0.000000              0.000000   \n",
       " \n",
       "       -0.013366278260946302  -0.012291878461837769  0.007085621356964084  \\\n",
       " 0                  0.007193              -0.001530             -0.009634   \n",
       " 1                  0.008134               0.007184             -0.017475   \n",
       " 2                  0.018260               0.004375             -0.037659   \n",
       " 3                 -0.091764               0.479898              0.242251   \n",
       " 4                  0.000000               0.000000              0.000000   \n",
       " ...                     ...                    ...                   ...   \n",
       " 2515               0.003836              -0.022679              0.040876   \n",
       " 2516              -0.244976               0.418520              0.730185   \n",
       " 2517               0.000000               0.000000              0.000000   \n",
       " 2518               0.000000               0.000000              0.000000   \n",
       " 2519               0.000000               0.000000              0.000000   \n",
       " \n",
       "       -0.017219804227352198  Unnamed: 63  \n",
       " 0                  0.009990          NaN  \n",
       " 1                  0.010624          NaN  \n",
       " 2                  0.021639          NaN  \n",
       " 3                 -0.112050          NaN  \n",
       " 4                  0.000000          NaN  \n",
       " ...                     ...          ...  \n",
       " 2515              -0.006501          NaN  \n",
       " 2516              -0.261835          NaN  \n",
       " 2517               0.000000          NaN  \n",
       " 2518               0.000000          NaN  \n",
       " 2519               0.000000          NaN  \n",
       " \n",
       " [2520 rows x 64 columns]]"
      ]
     },
     "metadata": {},
     "execution_count": 113
    }
   ],
   "source": [
    "#d1_train = dataset(df_list[0],label_list[0])\n",
    "d1_train = dataset(df_list,label_list[0])\n",
    "d1_train.lable\n",
    "d1_train.data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# import the libraries for classification modelling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, mean_squared_error\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNLSTMNet(nn.Module):\n",
    "    def __init__(self, cnn_model, output_dim, hidden_dim,num_classes=10,seq_len=20 ,batch_size=1, num_lstm_layers = 1, bidirectional = False, device = 'cpu', freeze_layers=True, dropout=0, title=\"default\"):\n",
    "        super(CNNLSTMNet, self).__init__()\n",
    "        # CNN\n",
    "        self.device = device\n",
    "        self.title = title # Model Title\n",
    "        self.cnn_model = cnn_model # Torchvision CNN Model\n",
    "        \n",
    "        # Optionally Freeze CNN Layers\n",
    "        if freeze_layers:\n",
    "            for idxc, child in enumerate(self.cnn_model.children()):\n",
    "                for param in child.parameters():\n",
    "                    param.requires_grad = False\n",
    "            self.cnn_model.fc.requires_grad = True\n",
    "            \n",
    "        # RNN\n",
    "        self.seq_len = seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_dim = hidden_dim # LSTM Hidden Dimension Size\n",
    "        self.num_lstm_layers = num_lstm_layers \n",
    "        self.bidirectional = bidirectional # Sets LSTM to Uni or Bidirectional\n",
    "        self.bidirectional_mult = 2 if self.bidirectional else 1 # Used for LSTM Weight Shape\n",
    "        self.lstm = nn.LSTM(output_dim, hidden_dim, self.num_lstm_layers, bidirectional=self.bidirectional, dropout=dropout)\n",
    "        self.hidden2class = nn.Linear(hidden_dim*self.bidirectional_mult, num_classes) # Fully Connected Output Layer\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # (num_layers, minibatch_size, hidden_dim)\n",
    "        return (torch.zeros(self.num_lstm_layers*self.bidirectional_mult, self.batch_size, self.hidden_dim).to(device),\n",
    "                    torch.zeros(self.num_lstm_layers*self.bidirectional_mult, self.batch_size, self.hidden_dim).to(device))\n",
    "                    \n",
    "    def forward(self, x):\n",
    "        x = x.view(self.seq_len*self.batch_size,x.shape[-3],x.shape[-2],-1)\n",
    "        out = self.cnn_model(x)\n",
    "        seq = out.view(self.batch_size, self.seq_len, -1).transpose_(0,1)\n",
    "        self.hidden = self.init_hidden()\n",
    "        # LSTM input shape = (seq_len, batch, input_size)\n",
    "        out, self.hidden = self.lstm(seq.view(len(seq), self.batch_size, -1), self.hidden)\n",
    "        #LSTM output shape = (seq_len, batch, hidden_dim * bidirectional)\n",
    "        out = self.hidden2class(out[-1])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_img_features = 1024 # CNN output dimensions\n",
    "num_epochs = 10\n",
    "sequence_len = d1_train.shape # LSTM sequence length\n",
    "batch_size=3\n",
    "hidden_dim = 128 # LSTM hidden dimension size\n",
    "lstm_dropout = .1\n",
    "lstm_depth = 1\n",
    "freeze = False # True = Freeze entire CNN, False = Don't freeze any layers\n",
    "pretrain = True # Use Imagenet Pretraining with CNN\n",
    "lstm_depth_title = 'no_freeze_layers_num_lstm_layers_'+str(lstm_depth)\n",
    "num_classes = len(label_list)+1\n",
    "loss_fn = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-dataset-test-frozen-False-num_img_features-1024-num_epochs-10-sequence_len-379-batch_size-3-lstm_dim-128-lstm_depth-1-pretrain-TrueCNN-resnet18\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "title =\\\n",
    "'-dataset-'+str(\"test\")+\\\n",
    "'-frozen-'+str(freeze)+\\\n",
    "'-num_img_features-'+ str(num_img_features) +\\\n",
    "'-num_epochs-'+str(num_epochs)+\\\n",
    "'-sequence_len-'+str(sequence_len)+\\\n",
    "'-batch_size-'+str(batch_size)+\\\n",
    "'-lstm_dim-'+str(hidden_dim)+\\\n",
    "'-lstm_depth-'+str(lstm_depth)+\\\n",
    "'-pretrain-'+str(pretrain)+\\\n",
    "'CNN-resnet18'\n",
    "\n",
    "cnn_model = models.resnet18(pretrained=pretrain) #Choose different CNN if desired\n",
    "num_ftrs = cnn_model.fc.in_features\n",
    "cnn_model.fc = nn.Linear(num_ftrs, num_img_features) # Change CNN output layer to desired dimension\n",
    "model = CNNLSTMNet(cnn_model, num_img_features, hidden_dim, num_classes, sequence_len, batch_size, num_lstm_layers=lstm_depth, bidirectional=False, device=device, freeze_layers=freeze, dropout=lstm_dropout)\n",
    "optimizer = optim.Adam(model.parameters(), lr = 1e-4)\n",
    "model_save_path= 'saved_models/'+title+'.pth'\n",
    "# Use Below to load train history and train over a saved model\n",
    "# model.load_state_dict(torch.load(model_save_path))\n",
    "print(title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "#from plot_model_stats import *\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torchnet\n",
    "from torchnet.meter import ConfusionMeter\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def my_collate(batch):\n",
    "    # Filters DataLoader items after they are retrieved\n",
    "    data = [item[0] for item in batch if item != None]\n",
    "    target = [item[1] for item in batch if item != None]\n",
    "    if len(data) == 0: return [None, None]\n",
    "    data = torch.stack(data)\n",
    "    target = torch.LongTensor(target)\n",
    "    return [data, target]\n",
    "\n",
    "\n",
    "def train_model(model, loss_fn, batch_size, dataset, optimizer, model_title='None', device='cpu', root_dir='', num_epochs = 2, testset=None):\n",
    "    def write_results(title, write_string):\n",
    "        with open('model_results/'+title + '_results.txt', 'a+') as f:\n",
    "            print(write_string)\n",
    "            f.write(write_string + '\\n')\n",
    "\n",
    "    # Shuffling is needed in case dataset is not shuffled by default.\n",
    "    train_ratio=.9\n",
    "    train_len = int(len(dataset)*train_ratio)\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_len, len(dataset)-train_len]) \n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                               batch_size=batch_size,\n",
    "                                               collate_fn=my_collate\n",
    "                                               )\n",
    "    # We don't need to bach the validation set but let's do it anyway.\n",
    "    val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                             batch_size=batch_size,\n",
    "                                             collate_fn=my_collate\n",
    "                                             )  \n",
    "\n",
    "    # GPU enabling.\n",
    "    model = model.to(device)\n",
    "    loss_fn = loss_fn.to(device)\n",
    "\n",
    "    # Training loop. Please make sure you understand every single line of code below.\n",
    "    # Go back to some of the previous steps in this lab if necessary.\n",
    "    start_time = time.time()\n",
    "    plot_path=os.path.join(root_dir,'model_results/'+model_title+'.png')\n",
    "    print(\"Trainset Length: \", len(train_loader))\n",
    "    print(\"Valset Length: \", len(val_loader))\n",
    "    print(\"Epoch Count: \", num_epochs)\n",
    "    print(\"Plot Path: \",plot_path)\n",
    "    \n",
    "    train_accuracies=[]; val_accuracies=[]; train_losses=[]; val_losses=[]\n",
    "    for epoch in range(0, num_epochs):\n",
    "        correct = 0.0\n",
    "        cum_loss = 0.0\n",
    "\n",
    "        # Make a pass over the training data.\n",
    "        model.train()\n",
    "        num_trained = 1.0\n",
    "        confusion_matrix = torch.zeros(len(label_list), len(label_list))\n",
    "        for (i, (inputs, labels)) in enumerate(train_loader):\n",
    "            try:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)#[0]\n",
    "\n",
    "                # Forward pass. (Prediction stage)\n",
    "                scores = model(inputs)\n",
    "                loss = loss_fn(scores, labels)\n",
    "\n",
    "                # Zero the gradients in the network.\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Backward pass. (Gradient computation stage)\n",
    "                loss.backward()\n",
    "\n",
    "                # Parameter updates (SGD step) -- if done with torch.optim!\n",
    "                optimizer.step()\n",
    "\n",
    "                scores = F.softmax(scores)\n",
    "                max_scores, max_labels = scores.max(1)\n",
    "                correct_eval = (max_labels == labels).sum().item()\n",
    "                correct+= correct_eval\n",
    "                num_trained+=batch_size\n",
    "                cum_loss += loss.item()\n",
    "                _, preds = torch.max(scores, 1)\n",
    "                for t, p in zip(labels.view(-1), preds.view(-1)):\n",
    "                    confusion_matrix[t.long(), p.long()] += 1\n",
    "\n",
    "\n",
    "                # Logging the current results on training.\n",
    "                if (i + 1) % 100 == 0:\n",
    "                    #print(dataset.labels)\n",
    "                    write_string =('Train-epoch %d. Iteration %05d, Avg-Loss: %.4f, Accuracy: %.4f' %\n",
    "                          (epoch, num_trained + 1, cum_loss / (num_trained), correct / (num_trained)))\n",
    "                    write_string += '\\n'+str(confusion_matrix)\n",
    "                    confusion_avg = (confusion_matrix.diag()/confusion_matrix.sum(1))\n",
    "                    write_string += '\\n'+ str(confusion_avg)\n",
    "                    write_results(model_title, write_string)\n",
    "                    #print(\"Time Elapsed Minutes: \", (time.time() - start_time) /60)\n",
    "                    correct = 0.0\n",
    "                    cum_loss = 0.0\n",
    "                    num_trained = 1.0\n",
    "                    confusion_matrix = torch.zeros(len(label_list), len(label_list))\n",
    "                    model_save_path= 'saved_models/'+model_title+'.pth'\n",
    "\n",
    "                    torch.save(model.state_dict(), model_save_path)\n",
    "            except Exception as e:\n",
    "                \n",
    "                print(e)\n",
    "        train_accuracies.append(correct / num_trained)\n",
    "        train_losses.append(cum_loss / num_trained)\n",
    "        write_string = \"Time Elapsed Minutes: \"+ str((time.time() - start_time) /60)\n",
    "        write_results(model_title, write_string)\n",
    "\n",
    "        # Make a pass over the validation data.\n",
    "        model.eval()\n",
    "        num_trained = 1.0\n",
    "        correct = 0.0\n",
    "        cum_loss = 0.0\n",
    "\n",
    "        confusion_matrix = torch.zeros(len(label_list), len(label_list))\n",
    "        print('Validating...')\n",
    "        #print(dataset.labels)\n",
    "        for (i, (inputs, labels)) in enumerate(val_loader):\n",
    "            try:\n",
    "  \n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)#[0]\n",
    "\n",
    "                # Forward pass. (Prediction stage)\n",
    "                scores = model(inputs)\n",
    "                latent_scores.append([scores.item(),labels.item()])\n",
    "                loss = loss_fn(scores, labels)\n",
    "                scores = F.softmax(scores)\n",
    "                # Count how many correct in this batch.\n",
    "                max_scores, max_labels = scores.max(1)\n",
    "                correct_eval = (max_labels == labels).sum().item()\n",
    "                correct+= correct_eval\n",
    "                num_trained+=batch_size\n",
    "                cum_loss += loss.item()\n",
    "                _, preds = torch.max(scores, 1)\n",
    "                for t, p in zip(labels.view(-1), preds.view(-1)):\n",
    "                    confusion_matrix[t.long(), p.long()] += 1\n",
    "                if i % 100 == 0:\n",
    "                    print(confusion_matrix)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "        val_accuracies.append(correct / num_trained)\n",
    "        val_losses.append(cum_loss / num_trained)\n",
    "        write_string =('validation-epoch %d. Iteration %05d, Avg-Loss: %.4f, Accuracy: %.4f' %\n",
    "              (epoch, num_trained + 1, cum_loss / num_trained, correct / num_trained ))\n",
    "        write_string += '\\n'+str(confusion_matrix)\n",
    "        confusion_avg = (confusion_matrix.diag()/confusion_matrix.sum(1))\n",
    "        write_string += '\\n'+ str(confusion_avg)\n",
    "        write_results(model_title, write_string)\n",
    "        # plot_model_stats(train_accuracies, val_accuracies, train_losses, val_losses, plot_path, latent_scores=latent_scores)\n",
    "        if testset != None:\n",
    "            try:\n",
    "                from test_model import test_model\n",
    "                test_model(model,testset,dataset, batch_size, joint_run=joint_run)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'all'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-117-557e12a745b2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;33m(\u001b[0m\u001b[0md1_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'all'"
     ]
    }
   ],
   "source": [
    " (d1_train.data).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Trainset Length:  2\nValset Length:  1\nEpoch Count:  10\nPlot Path:  model_results/-dataset-test-frozen-False-num_img_features-1024-num_epochs-10-sequence_len-379-batch_size-3-lstm_dim-128-lstm_depth-1-pretrain-TrueCNN-resnet18.png\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-116-6b327ae4adea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md1_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-115-808d94369cc4>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, loss_fn, batch_size, dataset, optimizer, model_title, device, root_dir, num_epochs, testset)\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[0mnum_trained\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[0mconfusion_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m                 \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    515\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 517\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    555\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    556\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 557\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    558\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-115-808d94369cc4>\u001b[0m in \u001b[0;36mmy_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmy_collate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;31m# Filters DataLoader items after they are retrieved\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-115-808d94369cc4>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmy_collate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;31m# Filters DataLoader items after they are retrieved\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__nonzero__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1440\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1441\u001b[1;33m         raise ValueError(\n\u001b[0m\u001b[0;32m   1442\u001b[0m             \u001b[1;34mf\"The truth value of a {type(self).__name__} is ambiguous. \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1443\u001b[0m             \u001b[1;34m\"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "train_model(model, loss_fn, batch_size, d1_train.data, optimizer, title, device, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}